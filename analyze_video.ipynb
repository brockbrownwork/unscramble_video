{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "476a117b-d0fe-4e7a-b5ba-e20763092ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating threshold videos: 100%|███████████████████████████████████████████████████████| 14/14 [02:43<00:00, 11.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitching all videos together...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stitching videos: 100%|████████████████████████████████████████████████████████████████| 14/14 [00:24<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined video saved as videos\\combined_thresholds_video.mp4.\n",
      "Deleting non-combined videos...\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.10.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.20.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.30.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.40.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.50.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.60.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.70.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.75.mp4\n",
      "Error: Could not delete videos\\euclidean_annotated_video_threshold_0.80.mp4. [WinError 32] The process cannot access the file because it is being used by another process: 'videos\\\\euclidean_annotated_video_threshold_0.80.mp4'\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.85.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.90.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.95.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.98.mp4\n",
      "Deleted videos\\euclidean_annotated_video_threshold_0.99.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "def compute_similarities(input_video_path, num_frames=200, verbose=False):\n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        if verbose:\n",
    "            print(\"Error: Could not open video.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        if verbose:\n",
    "            print(\"Failed to read the first frame of the video.\")\n",
    "        cap.release()\n",
    "        return None, None, None, None\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "    center_y = height // 2\n",
    "    center_x = width // 2\n",
    "\n",
    "    # Initialize array to store frames\n",
    "    frames_array = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Reading frames...\")\n",
    "\n",
    "    # Read the specified number of frames\n",
    "    frame_count = 0\n",
    "    while frame_count < num_frames:\n",
    "        if frame is None:\n",
    "            if verbose:\n",
    "                print(f\"Reached end of video at frame {frame_count}.\")\n",
    "            break\n",
    "        frames_array.append(frame)\n",
    "        ret, frame = cap.read()\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    num_frames = len(frames_array)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total frames read: {num_frames}\")\n",
    "\n",
    "    # Convert frames list to numpy array\n",
    "    frames_array = np.array(frames_array, dtype=np.uint8)  # Shape: (num_frames, height, width, 3)\n",
    "\n",
    "    # Reshape frames to (height * width, num_frames * channels)\n",
    "    if verbose:\n",
    "        print(\"Processing pixel time series...\")\n",
    "    frames_reshaped = frames_array.transpose(1, 2, 0, 3).reshape(height * width, num_frames * channels)\n",
    "\n",
    "    # Get the time series of the center pixel\n",
    "    center_index = center_y * width + center_x\n",
    "    center_pixel_vector = frames_reshaped[center_index, :]\n",
    "\n",
    "    # Compute Euclidean distances\n",
    "    if verbose:\n",
    "        print(\"Calculating Euclidean distances...\")\n",
    "    diffs = frames_reshaped.astype(np.float32) - center_pixel_vector.astype(np.float32)\n",
    "    distances = np.linalg.norm(diffs, axis=1)\n",
    "\n",
    "    # Normalize distances to get similarity (0 to 1)\n",
    "    if verbose:\n",
    "        print(\"Normalizing distances...\")\n",
    "    max_distance = distances.max()\n",
    "    if max_distance == 0:\n",
    "        if verbose:\n",
    "            print(\"Max distance is zero. All pixels are identical to the center pixel.\")\n",
    "        similarity = np.ones_like(distances)\n",
    "    else:\n",
    "        similarity = 1 - (distances / max_distance)\n",
    "\n",
    "    # Reshape similarity back to image shape\n",
    "    similarity_image = similarity.reshape(height, width)\n",
    "\n",
    "    return frames_array, similarity_image, fps, (height, width, channels), (center_x, center_y)\n",
    "\n",
    "def process_frames_for_threshold(frames_array, similarity_image, threshold, output_video_path, fps, frame_size, center_coords, dot_size=10):\n",
    "    height, width, channels = frame_size\n",
    "    center_x, center_y = center_coords\n",
    "    num_frames = frames_array.shape[0]\n",
    "\n",
    "    # Create mask for pixels based on threshold\n",
    "    white_mask = (similarity_image >= threshold).astype(np.float32)[:, :, np.newaxis]  # Shape: (height, width, 1)\n",
    "\n",
    "    # Normalize frames to [0,1] for blending with the white mask\n",
    "    blended_frames_float = frames_array.astype(np.float32) / 255.0\n",
    "    white_frame = np.ones((height, width, 3), dtype=np.float32)  # Fully white pixel overlay\n",
    "\n",
    "    # Blend the frames with white based on the mask\n",
    "    final_frames = blended_frames_float * (1.0 - white_mask) + white_frame * white_mask\n",
    "    final_frames = np.clip(final_frames * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Initialize VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        frame = final_frames[i]\n",
    "\n",
    "        # Add red dot at the center of the frame\n",
    "        cv2.circle(frame, (center_x, center_y), dot_size, (0, 0, 255), 1)\n",
    "\n",
    "        # Add the threshold text\n",
    "        text = f\"threshold: {threshold:.2f}\"\n",
    "        position = (width - 300, 50)  # Top-right position\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1\n",
    "        color = (0, 0, 0)  # Black color for text\n",
    "        thickness = 2\n",
    "        cv2.putText(frame, text.lower(), position, font, font_scale, color, thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "\n",
    "def main():\n",
    "    input_video_path = 'cab_ride_trimmed.mkv'\n",
    "    output_dir = 'videos'\n",
    "    num_frames = 200\n",
    "    verbose = False\n",
    "\n",
    "    # Compute similarities once\n",
    "    frames_array, similarity_image, fps, frame_size, center_coords = compute_similarities(\n",
    "        input_video_path, num_frames=num_frames, verbose=verbose)\n",
    "\n",
    "    if frames_array is None:\n",
    "        print(\"Failed to compute similarities.\")\n",
    "        return\n",
    "\n",
    "    # Create multiple videos for different thresholds\n",
    "    threshold_values = [0.99, 0.98, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "\n",
    "    for idx, threshold in enumerate(tqdm(threshold_values, desc=\"Creating threshold videos\")):\n",
    "        output_path = os.path.join(output_dir, f'euclidean_annotated_video_threshold_{threshold:.2f}.mp4')\n",
    "        current_dot_size = max(1, 10 - idx)  # Decrease dot size, ensuring it doesn't go below 1\n",
    "        process_frames_for_threshold(\n",
    "            frames_array, similarity_image, threshold, output_path, fps, frame_size, center_coords, dot_size=current_dot_size)\n",
    "\n",
    "    # Stitch all videos together\n",
    "    print(\"Stitching all videos together...\")\n",
    "    output_combined_path = os.path.join(output_dir, 'combined_thresholds_video.mp4')\n",
    "    video_files = sorted(glob.glob(os.path.join(output_dir, 'euclidean_annotated_video_threshold_*.mp4')))\n",
    "\n",
    "    # Get properties from the first video\n",
    "    cap = cv2.VideoCapture(video_files[0])\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open the first video for stitching.\")\n",
    "        return\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cap.release()\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_combined = cv2.VideoWriter(output_combined_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for video_file in tqdm(video_files, desc=\"Stitching videos\"):\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video {video_file}\")\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out_combined.write(frame)\n",
    "        cap.release()\n",
    "\n",
    "    out_combined.release()\n",
    "    print(f\"Combined video saved as {output_combined_path}.\")\n",
    "\n",
    "    # Delete the non-combined videos\n",
    "    print(\"Deleting non-combined videos...\")\n",
    "    for video_file in video_files:\n",
    "        try:\n",
    "            os.remove(video_file)\n",
    "            print(f\"Deleted {video_file}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error: Could not delete {video_file}. {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9192ec5-431f-4e6f-9032-46eef7b55b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
