Hi, I've been gnawing on this problem for a couple years and thought it would be fun to see if maybe other people are also interested in gnawing on it. The idea of doing this came from the thought that I don't think the positions of the "pixels" in our visual field are hard-coded, they are learned:

Take a video and treat each pixel position as a separate data stream (its RGB values over all frames). Now shuffle the positions of the pixels, without shuffling them over time. Think of plucking a pixel off of your screen and putting it somewhere else. Can you put them back without having seen the unshuffled video, or at least rearrange them close to the unshuffled version (rotated, flipped, a few pixels out of place)? I think this might be possible as long as the video is long, colorful, and widely varied because neighboring pixels in a video have similar color sequences over time. A pixel showing "blue, blue, red, green..." probably belongs next to another pixel with a similar pattern, not next to one showing "white, black, white, black...".

Well right now I'm calling "neighbor dissonance" the metric to focus on, where it tells you how related one pixel is to its surrounding positions. You want the arrangement that minimizes neighbor dissonance. I'm not sure how to formalize that but that is the notion. I've found that the metric that seems to work the best is taking the average of euclidean distances of the surrounding pixel position time series.
